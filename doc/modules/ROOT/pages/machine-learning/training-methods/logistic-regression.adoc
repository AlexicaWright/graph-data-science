[[machine-learning-training-methods-logistic-regression]]
[.beta]
= Logistic regression

include::partial$/operations-reference/beta-note.adoc[]

Logistic regression is a fundamental supervised machine learning classification method.
This trains a model by minimizing a loss function which depends on a weight matrix and on the training data.
The loss can be minimized for example using gradient descent.
In GDS we use the Adam optimizer which is a gradient descent type algorithm.

The weights are in the form of a `[c,d]` sized matrix `W` and a bias vector `b` of length `c`, where `d` is the feature dimension and `c` is equal to the number of classes.
The loss function is then defined as:

`CE(softmax(Wx + b))`

where `CE` is the https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression[cross entropy loss], `softmax` is the https://en.wikipedia.org/wiki/Softmax_function[softmax function], and `x` is a feature vector training sample of length `d`.

To avoid overfitting one may also add a https://en.wikipedia.org/wiki/Regularization_(mathematics)[regularization] term to the loss.
Neo4j Graph Data Science supports the option of `l2` regularization which can be configured using the `penalty` parameter.


include::partial$/machine-learning/training-methods/gradient-descent-config-tuning.adoc[leveloffset =+ 1]

include::partial$/machine-learning/training-methods/penalty-config-tuning.adoc[leveloffset =+ 1]

=== Focus weight

This parameter introduces the concept of _focal loss_, studied in https://arxiv.org/pdf/1708.02002v2.pdf[Focal Loss for Dense Object Detection], to the loss function of the logistic regression model.
When `focusWeight` is a value greater than zero, the loss function changes from standard Cross-Entropy Loss to Focal Loss, defined as:

image::equations/focal-loss.svg[align="center"]

where `p~t~` denotes the probability of the true class.
The `focusWeight` parameter is the exponent noted as Greek letter gamma.

Increasing `focusWeight` will guide the model towards trying to fit "hard" misclassified examples.
A hard misclassified example is an example for which the model has a low predicted probability for the true class.
In the above equation, the loss will be exponentially higher for low-true-class-probability examples, thus tuning the model towards trying to fit them, at the expense of potentially being less confident on "easy" examples.

In class-imbalanced datasets, the minority class(es) are typically harder to classify correctly.
Read more about class imbalance for Link Prediction in xref:machine-learning/linkprediction-pipelines/theory.adoc#linkprediction-pipelines-classimbalance[Class Imbalance].
